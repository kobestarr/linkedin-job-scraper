# LinkedIn Job Scraper MVP - Status

## âœ… Completed

### Core Modules Built

1. **Apify Integration** (`src/apify-client.js`)
   - Connects to Apify LinkedIn Job Scraper
   - Configurable actor ID (default: `n9WWs3eofIvboPcgK`)
   - Handles job scraping with filters (job title, location, date range)
   - Waits for run completion and retrieves dataset items

2. **Data Processor** (`src/data-processor.js`)
   - Parses raw Apify job data
   - Cleans and normalizes fields
   - Deduplicates by company name (keeps most recent)
   - Formats data for Google Sheets

3. **Google Sheets Client** (`src/google-sheets-client.js`)
   - Authenticates with Google Sheets API
   - Creates sheet if it doesn't exist
   - Sets headers automatically
   - Appends new jobs, skips duplicates
   - Preserves existing data

4. **Main Scraper** (`src/scraper.js`)
   - Orchestrates entire process
   - Apify â†’ Process â†’ Google Sheets
   - Command-line support with overrides
   - Comprehensive logging

5. **Scheduler** (`src/scheduler.js`)
   - Daily automated runs via node-cron
   - Supports multiple job titles (discrete runs)
   - Configurable schedule and timezone
   - Graceful shutdown handling

### Project Structure

```
/root/linkedin-job-scraper/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ apify-client.js
â”‚   â”œâ”€â”€ data-processor.js
â”‚   â”œâ”€â”€ google-sheets-client.js
â”‚   â”œâ”€â”€ scraper.js
â”‚   â””â”€â”€ scheduler.js
â”œâ”€â”€ credentials/          # Google Sheets credentials (gitignored)
â”œâ”€â”€ config.json           # Your config (gitignored)
â”œâ”€â”€ config.example.json   # Example config
â”œâ”€â”€ package.json
â”œâ”€â”€ index.js              # Entry point
â”œâ”€â”€ README.md
â””â”€â”€ SETUP.md
```

### Features Implemented

- âœ… One job title per run (discrete)
- âœ… Country-level location filtering
- âœ… Last 24 hours filtering
- âœ… Automatic deduplication by company
- âœ… Direct push to Google Sheets (no approval)
- âœ… Daily scheduler support
- âœ… Error handling and logging
- âœ… Command-line overrides

## ğŸ”§ Next Steps (Setup Required)

### 1. Apify Setup
- [ ] Sign up for Apify account
- [ ] Get API token from https://console.apify.com/account/integrations
- [ ] Add token to `config.json`

### 2. Google Sheets Setup
- [ ] Create Google Cloud Project
- [ ] Enable Google Sheets API
- [ ] Create Service Account
- [ ] Download credentials JSON
- [ ] Place in `credentials/google-sheets-credentials.json`
- [ ] Create Google Sheet
- [ ] Share sheet with service account email
- [ ] Add spreadsheet ID to `config.json`

### 3. Configuration
- [ ] Copy `config.example.json` to `config.json`
- [ ] Add Apify API token
- [ ] Add Google Sheets spreadsheet ID
- [ ] Set job title to scrape
- [ ] Configure schedule (if using scheduler)

### 4. Testing
- [ ] Test manual run: `node src/scraper.js`
- [ ] Verify jobs appear in Google Sheets
- [ ] Test deduplication (run twice, should skip duplicates)
- [ ] Test scheduler: `npm start`

## ğŸ“‹ Usage Examples

### Manual Run
```bash
cd /root/linkedin-job-scraper
node src/scraper.js
```

### Override Job Title
```bash
node src/scraper.js --job-title "Public Relations Manager" --location "United States"
```

### Start Scheduler
```bash
npm start
# Runs jobs based on schedule in config.json
```

## ğŸ¯ MVP Success Criteria

- [ ] System runs daily without manual intervention
- [ ] Jobs scraped within last 24 hours appear in Google Sheets
- [ ] Deduplication reduces duplicate companies
- [ ] Leads appear directly in Sheets for team review
- [ ] Process is faster than manual scraping

## ğŸ“ Notes

- Default Apify actor: `n9WWs3eofIvboPcgK` (Apify Linkedin Job Scrapper [NO COOKIES])
- Default schedule: Daily at 10 AM Eastern Time
- Default max results: 50 jobs per run
- All logs go to console (can redirect to file if needed)

## ğŸš€ Ready for Testing

The MVP is complete and ready for testing once credentials are configured!
